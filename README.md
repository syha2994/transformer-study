# transformer-study
트랜스포머의 기초와 구조를 공부하기 위해 만들어진 프로젝트입니다.


---
### 트랜스포머 아키텍처 기본 구조

----
![Transformer Architecture](./images/transformer_basic_architecture.png)

* 임베딩 층 : 데이터의 의미를 컴퓨터가 이해할 수 있도록 숫자의 집합으로 표현한 것.
  * 1단계 토큰화
    * 토큰화는 텍스트를 적절한 단위로 잘라 아이디를 부여하는 것임.
    * 토큰화는 단위가 작을수록 텍스트의 의미가 사라진다는 문제가 있고, 단위가 클수록 의미가 잘 유지되지만 사전에 없는 단어 (OOV)문제가 발생할 수 있음.
    * ![Transformer Architecture](./images/tokenizer.png)
  * 2단계 토큰 임베딩으로 변환
    * 토큰화에서 부여한 토큰 아이디는 하나의 숫자 집합일 뿐 토큰의 의미를 담을 수 없음.
    * 의미를 담기 위해선 최소 두 개 이상의 숫자 집합인 벡터여야 함.
* 위치 인코딩 층: 문장의 위치 정보를 더함
  * 트랜스포머는 모든 입력을 동시에 처리하기 때문에 순서 정보가 사라짐
  * 순서는 매우 중요한 정보이기 때문에 위치 정보를 추가하는 것.
* 인코더: 언어를 이해하는 역할
* 디코더: 언어를 생성하는 역할


### 어텐션 이해하기

----
* 사전적인 의미:주의
* 텍스트를 처리하는 관점: 입력한 텍스트에서 어떤 단어가 서로 관련되는지 "주의를 기울여 파악한다는 의미"
* 사람이 글을 읽는 방법과 어텐션
  * 사람은 왼쪽에서 오른쪽으로 흐르듯이 읽지만
  * 복잡하고 어려운 글을 읽을 때에는 자꾸 멈추면서 어떤 단어와 어떤 단어가 연결되는지 문장 안에서 고민하기도 하고
  * 문장간에 찾아보기도 함
  * 어텐션은 이렇게 사람이 단어 사이의 관계를 고민하는 과정을 딥러닝 모델이 수행할 수 있도록 모방한 연산
  

### 쿼리, 키, 값 이해하기

----
* 쿼리
  * 검색창에 검색을 할 때 우리가 입력하는 검색어
* 키
  * 쿼리를 입력하고 엔터를 치면 검색 엔진은 수많은 자료 중 쿼리와 관련있는 문서를 찾음
  * 이 때 쿼리와 관련이 있는지 계산하기 위해 문서가 가진 특징을 키라고 함. (ex) 문서 제목, 문서 본문, 저자 이름 등)
  * 값: 검색 엔진이 쿼리와 관련 깊은 키를 가진 문서를 찾아 관련도 순으로 정렬해서 문서를 제공할 때 문서를 값이라 함.


### 멀티 헤드 어텐션 이해하기
* 여러 어텐션 연산을 동시에 적용하면 성능을 더 높일 수 있음.
* 토큰 사이의 관계를 한 가지 측면에서 이해하는 것보다, 여러 측면을 동시에 고려할 때 언어나, 문장에 대한 이해도가 높아질 것.
* 스케일 정곱 어텐션
  * 입력 Q, K, V
  * 과정
    * Q와 K를 내적해서 유사도 계산
    * 스케일링 (값이 너무 커지지 않도록 나눠줌)
    * 마스킹 (optional)
    * 소프트맥스: 관련도를 확률처럼 정규화
    * 그 확률로 V를 가중합 -> 최종 결과
  * 즉, 한 번의 계산만 이루어짐
* **멀티 헤드 어텐션**
  * 입력 Q, K, V
  * 과정
    * 여러 개의 Linear Layer를 거쳐서 h개의 서로 다른 Q, K, V로 변환
    * 각각의 어텐션을 병렬로 h번 수행
    * 그 결과를 모아서 (cancatenate) 다시 한 번 선형층을 통과시켜 최종 출력 생성
    * 결과: 여러 관점에서 단어간 관련도를 파악함으로써 문맥을 더 잘 이해하게 됨.


### 인코더

----
* 멀티 헤드 어텐션, 층 정규화, 피드 포워드 층이 반복되는 형태
* resiual connection 활용
* 멀티 헤드 어텐션
  * 여러 시선으로 문장 안의 단어 관계 파악
* 층 정규화
  * 어텐션 결과를 정규화서 모델 학습 안정화
* 피드 포워드 층
  * 어텐션 결과를 각 단어 별로 개별 처리하는 작은 신경망
* 층 정규화(한 번 더)
  * 피드포워드 층 결과도 정규화
* 잔차 연결
  * 원래 입력을 중간 결과에 더해서 정보 손실 없이 전달되게 함.


### 디코더

----
* 마스크드 멀티 헤드 어텐션
  * 지금까지 생성된 단어만 보고 어텐션을 계산
  * 아직 생성되지 않은 미래 단어는 가려서(masking) 못 보게 막음
* 크로스 어텐션
  * 인코더의 출력과 연결되어 있음
  * 인코더가 입력 문장에서 뽑은 정보를 참조해서 더 좋은 출력을 생성할 수 있게 해줌
* 피드포워드 층
* 핵심 아이디어: 이미 생성한 단어들만 보면서, 인코더가 뽑아준 정보를 찹고해 다음 단어를 하나씩 만들어가는 구조
